{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "assert os.environ[\"CONDA_DEFAULT_ENV\"] == \"rekall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/share/pi/cleemess/eeg-summaries/good_video_lpch/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the video metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open('/share/pi/cleemess/eeg-summaries/metadata.json') as f:\n",
    "    video_metadata = list(json.load(f))\n",
    "video_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = [vm for vm in video_metadata if vm['filename'].split(\"/\")[0] in ['BA12305R', 'BA12306N']]\n",
    "video_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata[0]['filename'].split(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vm in video_metadata:\n",
    "    fn = vm[\"filename\"]\n",
    "    json_fn = os.path.join(BASE_DIR, fn.replace(\".mp4\", \".json\"))\n",
    "    print(fn, json_fn, os.path.isfile(json_fn))\n",
    "    lines = {}\n",
    "    with open(json_fn) as f:\n",
    "        for i, line in tqdm(enumerate(list(f.readlines()))):\n",
    "            line = json.loads(line)\n",
    "            if len(line):\n",
    "                lines[i] = line\n",
    "                prev_line = line\n",
    "    print(lines)\n",
    "    print(i)\n",
    "    break\n",
    "#     print(new_fn)\n",
    "#     print(vm[\"filename\"], os.listdir(\"/share/pi/cleemess/eeg-summaries/good_video_lpch/\" + vm['filename'].split(\"/\")[0] + \"/VOR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} videos with metadata'.format(len(video_metadata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import eeghdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out videos that we can't find the annotation file for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_exists_count = 0\n",
    "for i, vm in enumerate(video_metadata):\n",
    "#     print(vm)\n",
    "    # Convert video filename to annotation filename\n",
    "    vm[\"filename\"] = vm[\"filename\"].replace(\"stanford/\",\"\")\n",
    "    fn = vm[\"filename\"]\n",
    "    intermediate = fn.split(\"/\")[-1]\n",
    "    num = int(intermediate[-6:-4])\n",
    "    parent = \"/\".join(fn.split(\"/\")[:-2])\n",
    "    img_folder = fn.split(\"/\")[-2][:-4]\n",
    "    annotation_file = img_folder + f\"_1-{num+1}+.eeg.h5\"\n",
    "    full_fn = os.path.join(\"/share/pi/cleemess/eeg-summaries/good_video_lpch/\", parent, f\"{parent}{annotation_file}\")\n",
    "#     if not os.path.isfile(full_fn):\n",
    "#         print(fn, full_fn, os.path.isfile(full_fn))\n",
    "    \n",
    "    vm[\"annotation_filename\"] = full_fn if os.path.exists(full_fn) else None\n",
    "    annotation_exists_count += int(os.path.exists(full_fn))\n",
    "    \n",
    "print(f\"We found annotations for {annotation_exists_count} of the {len(video_metadata)} videos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter out videos that aren't broken up into pieces because we aren't totally sure how they line back up together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_video_count = 0\n",
    "for vm in video_metadata:\n",
    "    fn = vm[\"filename\"]\n",
    "    parent = os.path.join(\"/share/pi/cleemess/eeg-summariees\", \"/\".join(fn.split(\"/\")[:-1]))\n",
    "    fn = fn.split(\"/\")[-1]\n",
    "    other_vids = glob(os.path.join(parent, fn[:-6]) + \"*.mp4\")\n",
    "    vm[\"only_video\"] = len(other_vids) == 1\n",
    "    only_video_count += int(len(other_vids) == 1)\n",
    "print(f\"{only_video_count} of the {len(video_metadata)} videos are the only video in their directory (so this video is not a part of a sequence of broken up clips).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we find out the number of videos we can actually work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for vm in video_metadata:\n",
    "    count += int(vm[\"only_video\"] and vm[\"annotation_filename\"] is not None)\n",
    "print(f\"{count} of the {len(video_metadata)} videos are both the only video in their directory and have annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize videos with annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgrid import VGridSpec, VideoMetadata, VideoBlockFormat, SpatialType_Caption\n",
    "from vgrid_jupyter import VGridWidget\n",
    "from rekall import Interval, IntervalSet, IntervalSetMapping, Bounds3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the video metadata in a special object for use in VGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata_wrapper = [\n",
    "    VideoMetadata(\n",
    "        vm[\"filename\"], id=str(id), fps=vm[\"fps\"],\n",
    "        num_frames=int(vm[\"num_frames\"]), width=vm[\"width\"], height=vm[\"height\"])\n",
    "    for id, vm in enumerate(video_metadata)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a  function that filters the videos for annotations that contain the keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clips_for_keyword(keyword, use_only_video=False):\n",
    "    ism = {}\n",
    "    for vm in tqdm(video_metadata):\n",
    "        if not vm[\"annotation_filename\"] or (not vm[\"only_video\"] and use_only_video):\n",
    "            continue\n",
    "        try:\n",
    "            h5 = eeghdf.Eeghdf(vm[\"annotation_filename\"])\n",
    "        except:\n",
    "            print(vm[\"annotation_filename\"])\n",
    "            os.remove(vm[\"annotation_filename\"])\n",
    "            continue\n",
    "        starts = [start / 10**7 for start in h5._annotation_start100ns]\n",
    "        texts = h5._annotation_text\n",
    "\n",
    "        if not keyword or any(keyword.lower() in text.lower() for text in texts):\n",
    "            interval_set = IntervalSet([\n",
    "                    Interval(\n",
    "                        Bounds3D(start , start + 5), # we set the duration\n",
    "                        {\n",
    "                            'spatial_type': SpatialType_Caption(\">>\" + text + \"\\n\"),\n",
    "                            'metadata': {}\n",
    "                        }\n",
    "                    ) for start, text in zip(starts, texts)\n",
    "                ]) \n",
    "            ism[vm[\"id\"]] = interval_set\n",
    "\n",
    "    print(f\"Found {len(ism)} videos with keyword {keyword} in the annotations.\")\n",
    "    vgrid_spec = VGridSpec(\n",
    "        video_meta=video_metadata_wrapper,\n",
    "        vis_format=VideoBlockFormat(imaps=[('bboxes', ism)]),\n",
    "        video_endpoint='http://localhost:8080'\n",
    "    )\n",
    "    return VGridWidget(vgrid_spec=vgrid_spec.to_json_compressed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the function to visualize clips with seizures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_clips_for_keyword(\"seizure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_clips_for_keyword(\"patting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the videos are not as good, but the first video clearly shows a seizure at the annotation's time stamp. \n",
    "\n",
    "The final video also shows a seizure at the 9:40 mark, not where the annotation start time marker is though.\n",
    "\n",
    "The other videos don't seem like seizures to me but I am by no means an expert."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
